{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG-Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Libraries Importing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp - Only for DEV!\n",
    "# !pip install langchain-huggingface\n",
    "# !pip install langchain-qdrant\n",
    "# # !pip install qdrant-client\n",
    "# !pip install openai\n",
    "# !pip install mlflow\n",
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWtZu1owcWyu",
    "outputId": "ba609011-dcc2-445f-94ab-d26e0f180116"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "if 'nltk_data' in os.listdir('.'):\n",
    "    print('NLTK data already downloaded!')\n",
    "else:\n",
    "    nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import openai\n",
    "import mlflow\n",
    "from mlflow import MlflowClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Global Variables Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_DIRNAME = \"RAG_artifacts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Ingestion Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_URL = \"https://blog.dzencode.com/ru/illyuziya-kachestva-vash-sayt-idealen-pozdravlyaem-vy-tolko-chto-sozhgli-byudzhet/\"\n",
    "ARTICLE_TXT_FILENAME = 'article.txt'\n",
    "ARTICLE_METADATA_FILENAME = 'metadata.json'\n",
    "ARTICLE_METADATA_URL_KEY = 'url'\n",
    "ARTICLE_METADATA_LANG_KEY = 'lang'\n",
    "ARTICLE_METADATA_DATE_KEY = 'date'\n",
    "ARTICLE_METADATA_TOPIC_KEY = 'topic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Cleaning and Preprocessing Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_ARTICLE_TEXT_FILENAME = 'cleaned_article.txt'\n",
    "ARTICLE_LANGUAGE = 'russian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Chunking Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS_JSONL_FILENAME = 'rag_article.jsonl'\n",
    "CHUNK_SIZE_IN_TOKENS = 512\n",
    "CHUNK_METADATA_ID_KEY = 'chunk_ID'\n",
    "CHUNK_DATA_TEXT_KEY = 'chunk_text'\n",
    "CHUNK_DATA_METADATA_KEY = 'metadata'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chunks Embedding Generation and Loading into QdrantDB Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TRANSFORMER_MODEL_NAME = \"multi-qa-distilbert-cos-v1\"\n",
    "CHUNK_DATA_EMBEDDING_VECTOR_KEY = 'embedding'\n",
    "CHUNKS_COLLECTION_NAME = \"RAG_Chunks\"\n",
    "EMBEDDING_VECTOR_SIZE = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL_NAME).get_sentence_embedding_dimension()\n",
    "QDRANT_HOST = os.environ.get('QDRANT_DB_IP')\n",
    "QDRANT_PORT = \"6333\"\n",
    "QDRANT_CLIENT = QdrantClient(host = QDRANT_HOST, port = QDRANT_PORT)\n",
    "EMBEDDINGS_MODEL = HuggingFaceEmbeddings(model_name = f\"sentence-transformers/{SENTENCE_TRANSFORMER_MODEL_NAME}\")\n",
    "EMBEDDING_VECTORS_FILENAME = \"embeddings.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Chunks Searching Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_QUERY = \"Что автор имеет в виду под \\\"иллюзией качества\\\"?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prompt Generation for LLM Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_TASK_TEMPLATE = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Use the following pieces of retrieved context to answer the question:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLM Response Generation Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMACPP_SERVER_HOST = os.environ.get('LLAMACPP_Server_IP')\n",
    "LLAMACPP_SERVER_PORT = \"8080\"\n",
    "LLM_MODEL_NAME = \"mistral-v7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Control and Accounting (MLFlow)Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_HOST = os.environ.get(\"MLFLOW_IP\")\n",
    "MLFLOW_TRACKING_PORT = \"5000\"\n",
    "MLFLOW_TRACKING_URI = f\"http://{MLFLOW_TRACKING_HOST}:{MLFLOW_TRACKING_PORT}\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"embedding_model_for_RAG_experiment\"\n",
    "RAG_VERSIONS_JOURNAL_NAME = \"version_log.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup MLFlow Experiment and Start RAG Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ARTIFACTS_DIRNAME in os.listdir():\n",
    "    print('Artifacts dir already exists!')\n",
    "else:\n",
    "    print('Creating RAG Artifacts dir...')\n",
    "    os.system(\"mkdir RAG_artifacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow_client = mlflow.client.MlflowClient(MLFLOW_TRACKING_URI)\n",
    "\n",
    "if mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME):\n",
    "    print(f'Experiment {MLFLOW_EXPERIMENT_NAME} already exists!')\n",
    "    experiment_ID = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME).experiment_id\n",
    "else:\n",
    "    experiment_ID = mlflow.create_experiment(MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "experiment_existed_parent_runs = mlflow.search_runs(\n",
    "    experiment_ID, \n",
    "    filter_string = \"\"\"tags.run_type = 'parent'\"\"\",\n",
    "    output_format = 'list'\n",
    ")\n",
    "\n",
    "def get_current_parent_run_tags(experiment_existed_parent_runs, parent_runs_versions):\n",
    "    current_parent_run_tags = {\n",
    "        'run_type': 'parent',\n",
    "        'parent_version': 0,\n",
    "        'artifacts_dirname': ARTIFACTS_DIRNAME,\n",
    "        'article_URL': ARTICLE_URL,\n",
    "        'preprocessed_article_text_filename': PREPROCESSED_ARTICLE_TEXT_FILENAME,\n",
    "        'article_lang': ARTICLE_LANGUAGE,\n",
    "        'chunk_size': str(CHUNK_SIZE_IN_TOKENS),\n",
    "        'embedding_model_name': SENTENCE_TRANSFORMER_MODEL_NAME,\n",
    "        'chunks_collection_name': CHUNKS_COLLECTION_NAME,\n",
    "        'embedding_vector_size': str(EMBEDDING_VECTOR_SIZE),\n",
    "        'user_query': USER_QUERY,\n",
    "        'llm_task_template': LLM_TASK_TEMPLATE,\n",
    "        'llm_model_name': LLM_MODEL_NAME\n",
    "    }\n",
    "    if experiment_existed_parent_runs == []:\n",
    "        print('This is new experiment with 0 parent runs.')\n",
    "        print('Setting current parent run version to 1.')\n",
    "        current_parent_run_version = 1\n",
    "    else:\n",
    "        current_parent_run_version = parent_runs_versions[-1] + 1\n",
    "    current_parent_run_tags['parent_version'] = str(current_parent_run_version)\n",
    "    return current_parent_run_tags\n",
    "\n",
    "def get_parent_runs_versions(parent_runs: list, experiment_name):\n",
    "    experiment_artifacts_URI = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME).artifact_location\n",
    "    parent_runs_versions = []\n",
    "    for parent_run in parent_runs:\n",
    "        parent_run_version = parent_run.data.tags['parent_version']\n",
    "        parent_runs_versions.append(parent_run_version)\n",
    "    return parent_runs_versions\n",
    "\n",
    "parent_runs_versions = get_parent_runs_versions(experiment_existed_parent_runs, MLFLOW_EXPERIMENT_NAME)\n",
    "\n",
    "current_parent_run_tags = get_current_parent_run_tags(experiment_existed_parent_runs, parent_runs_versions)\n",
    "parent_run_version = current_parent_run_tags['parent_version']\n",
    "current_parent_run_name = f\"RAG_pipeline_execution_version_#{parent_run_version}\"\n",
    "\n",
    "mlflow.start_run(\n",
    "    experiment_id = experiment_ID,\n",
    "    run_name = current_parent_run_name,\n",
    "    tags = current_parent_run_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UebbNzZ8czJr"
   },
   "source": [
    "## **Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRBw74c8bIit"
   },
   "outputs": [],
   "source": [
    "def main(URL: str):\n",
    "    article_metadata = {\n",
    "        ARTICLE_METADATA_URL_KEY: URL,\n",
    "        ARTICLE_METADATA_LANG_KEY: '',\n",
    "        ARTICLE_METADATA_DATE_KEY: '',\n",
    "        ARTICLE_METADATA_TOPIC_KEY: ''\n",
    "    }\n",
    "    response = requests.get(URL)\n",
    "    if response.status_code == 200:\n",
    "        article_html = get_article_html(response)\n",
    "        html_parser = BeautifulSoup(article_html, 'html.parser')\n",
    "        article_metadata[ARTICLE_METADATA_LANG_KEY] = get_article_lang(response)\n",
    "        article_metadata[ARTICLE_METADATA_DATE_KEY] = get_request_date(response)\n",
    "        article_metadata[ARTICLE_METADATA_TOPIC_KEY] = get_article_topic(html_parser)\n",
    "        save_metadata(article_metadata)\n",
    "        save_article_HTML_to_txt(article_html)\n",
    "    else:\n",
    "        print(f\"Error fetching data from API. Status code: {response.status_code}\")\n",
    "        print(f\"Response:\", response.text)\n",
    "\n",
    "def get_article_lang(response):\n",
    "    cookie_str = response.headers['Set-Cookie']\n",
    "    lang_regex = re.compile(r\"(USER_LANG=)([a-zA-Z]*);\")\n",
    "    lang_match_obj = lang_regex.search(cookie_str)\n",
    "    lang = lang_match_obj.group(2)\n",
    "    return lang\n",
    "\n",
    "def get_request_date(response):\n",
    "    return response.headers['Date']\n",
    "\n",
    "def get_article_topic(html_parser):\n",
    "    topic = html_parser.find('title').text\n",
    "    return topic\n",
    "\n",
    "def get_article_html(response):\n",
    "    return response.text\n",
    "\n",
    "def save_metadata(metadata: dict):\n",
    "    with open(f\"{ARTIFACTS_DIRNAME}/{ARTICLE_METADATA_FILENAME}\", 'w') as mdf:\n",
    "        json.dump(metadata, mdf)\n",
    "\n",
    "def save_article_HTML_to_txt(article_html):\n",
    "    with open(f\"{ARTIFACTS_DIRNAME}/{ARTICLE_TXT_FILENAME}\", 'w') as article_f:\n",
    "        article_f.write(article_html)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Data_Ingestion\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    main(ARTICLE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAj10fZ4eF1m"
   },
   "source": [
    "## **Data Cleaning and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJ-y2XH8dG2a"
   },
   "outputs": [],
   "source": [
    "loaded_stopwords = stopwords.words(ARTICLE_LANGUAGE)\n",
    "\n",
    "def main(article_filename: str):\n",
    "\tarticle_html = get_article_html(article_filename)\n",
    "\tarticle_without_html_tags = remove_html_tags(article_html)\n",
    "\tpreprocessed_article_text = get_cleaned_preprocessed_article_text(article_without_html_tags)\n",
    "\tsave_preprocessed_article_text(preprocessed_article_text)\n",
    "\n",
    "def remove_html_tags(article_html: str):\n",
    "\thtml_parser = BeautifulSoup(article_html, 'html.parser')\n",
    "\tonly_article_text = html_parser.find('html').text\n",
    "\treturn only_article_text\n",
    "\n",
    "def get_cleaned_preprocessed_article_text(article_text: str):\n",
    "\tnot_word_chars_digit_regex = r'[\\W0-9]'\n",
    "\tlowered_article_text = article_text.lower()\n",
    "\tarticle_text_elements = re.split(not_word_chars_digit_regex, lowered_article_text)\n",
    "\tarticle_text_without_empty_strs_stopwords = [\n",
    "\t\tword for word in article_text_elements if word != '' and word not in loaded_stopwords\n",
    "\t]\n",
    "\tpreprocessed_article_text = ' '.join(article_text_without_empty_strs_stopwords)\n",
    "\treturn preprocessed_article_text\n",
    "\n",
    "def get_article_html(article_filename: str):\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{article_filename}\", 'r') as article_f:\n",
    "\t\tarticle_html = article_f.read()\n",
    "\treturn article_html\n",
    "\n",
    "def save_preprocessed_article_text(preprocessed_text: str):\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{PREPROCESSED_ARTICLE_TEXT_FILENAME}\", 'w') as prep_txt_f:\n",
    "\t\tprep_txt_f.write(preprocessed_text)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Data_Cleaning_and_Preprocessing\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    main(ARTICLE_TXT_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGCAa5G_frAy"
   },
   "source": [
    "**Data Chunking for RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q54Zn8ZdeM1i"
   },
   "outputs": [],
   "source": [
    "def main(preprocessed_article_file: str, chunk_size_in_tokens: int, metadata_file: str, chunks_JSONL_file: str):\n",
    "    preprocessed_article_text = get_preprocessed_article_text(preprocessed_article_file)\n",
    "    article_words = get_article_words(preprocessed_article_text)\n",
    "    chunks_texts = get_chunks_texts(article_words, chunk_size_in_tokens)\n",
    "    metadata = get_metadata(metadata_file)\n",
    "    chunked_data = get_chunked_data(chunks_texts, metadata)\n",
    "    save_chunks_into_JSONL(chunked_data, chunks_JSONL_file)\n",
    "\n",
    "def get_preprocessed_article_text(filename: str):\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{filename}\", 'r') as prep_txt_f:\n",
    "\t\tpreprocessed_article_text = prep_txt_f.read()\n",
    "\t\treturn preprocessed_article_text\n",
    "\n",
    "def get_article_words(article_text: str) -> list:\n",
    "\treturn article_text.split(' ')\n",
    "\n",
    "def get_chunked_data(chunks_texts: list[str], metadata: dict):\n",
    "    current_chunk_ID = 1\n",
    "    chunked_data = []\n",
    "    for (chunk_ID, chunk_text) in enumerate(chunks_texts, current_chunk_ID):\n",
    "        chunk_metadata = metadata.copy()\n",
    "        chunk_metadata[CHUNK_METADATA_ID_KEY] = chunk_ID\n",
    "        chunk = {CHUNK_DATA_TEXT_KEY: '', CHUNK_DATA_METADATA_KEY: chunk_metadata}\n",
    "        chunk[CHUNK_DATA_TEXT_KEY] = chunk_text\n",
    "        chunked_data.append(chunk)\n",
    "    return chunked_data\n",
    "\n",
    "def get_chunks_texts(article_words: list, words_symbols_amount_for_chunks: int):\n",
    "    chunks_texts = []\n",
    "    while len(article_words) != 0:\n",
    "        chunk_words_remained_article_words = get_chunk_words(article_words, words_symbols_amount_for_chunks)\n",
    "        chunks_texts.append(' '.join(chunk_words_remained_article_words['chunk_words']))\n",
    "        article_words = chunk_words_remained_article_words['remained_article_words']\n",
    "    return chunks_texts\n",
    "        \n",
    "def get_chunk_words(article_words: list, words_symbols_amount_for_chunks: int):\n",
    "    chunk_words = []\n",
    "    while len(''.join(chunk_words)) < words_symbols_amount_for_chunks:\n",
    "        if len(''.join(article_words)) - words_symbols_amount_for_chunks < words_symbols_amount_for_chunks:\n",
    "            chunk_words = article_words\n",
    "            return {'chunk_words': chunk_words, 'remained_article_words': []}\n",
    "        chunk_words.append(article_words.pop(0))\n",
    "    return {'chunk_words': chunk_words, 'remained_article_words': article_words}\n",
    "\n",
    "def get_metadata(metadata_file: str):\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{metadata_file}\", 'r') as metadata_f:\n",
    "\t\treturn json.load(metadata_f)\n",
    "\n",
    "def save_chunks_into_JSONL(chunks: list, JSONL_file: str):\n",
    "\tserialized_jsonl_chunks = '\\n'.join([json.dumps(chunk) for chunk in chunks])\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{JSONL_file}\", 'w') as jsonl_f:\n",
    "\t\tjsonl_f.write(serialized_jsonl_chunks)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Data_Chunking_for_RAG\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    main(PREPROCESSED_ARTICLE_TEXT_FILENAME,\n",
    "    \tCHUNK_SIZE_IN_TOKENS,\n",
    "    \tARTICLE_METADATA_FILENAME,\n",
    "    \tCHUNKS_JSONL_FILENAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings Generation and Loading into Qdrant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(qdrant_client, chunks_collection_name, \n",
    "chunks_file: str, embed_model, embed_vector_size, embed_vectors_filename):\n",
    "    chunks = get_chunked_data(chunks_file)\n",
    "    if qdrant_client.collection_exists(chunks_collection_name):\n",
    "        print(\"Qdrant Collection for Chunks Embeddings already exists!\")\n",
    "        print(f\"Deleting {chunks_collection_name} collection...\", end = '')\n",
    "        qdrant_client.delete_collection(chunks_collection_name)\n",
    "        print(\"DONE\")\n",
    "    print(\"Collection doesn't exist. Creating...\", end = '')\n",
    "    create_chunks_Qdrant_collection(qdrant_client, chunks_collection_name, embed_vector_size)\n",
    "    print('DONE')\n",
    "    chunks_texts_embedding_vectors = get_chunks_texts_embed_vectors(embed_model, chunks)\n",
    "    save_chunks_embedding_vectors(chunks_texts_embedding_vectors, embed_vectors_filename)\n",
    "    add_embed_vectors_to_chunks(chunks, chunks_texts_embedding_vectors)\n",
    "    add_chunks_to_Qdrant_collection(embed_model, chunks, qdrant_client, chunks_collection_name)\n",
    "\n",
    "def get_chunked_data(chunks_file: str) -> list[dict]:\n",
    "\tloaded_chunked_data = []\n",
    "\twith open(f\"{ARTIFACTS_DIRNAME}/{chunks_file}\", 'r') as chunks_f:\n",
    "\t\tfor line in chunks_f:\n",
    "\t\t\tloaded_chunked_data.append(json.loads(line))\n",
    "\treturn loaded_chunked_data\n",
    "\n",
    "def create_chunks_Qdrant_collection(client, chunks_collection_name: str, embed_vector_size: int):\n",
    "    client.create_collection(\n",
    "        collection_name = chunks_collection_name,\n",
    "        vectors_config = VectorParams(\n",
    "            size = embed_vector_size,\n",
    "            distance = Distance.DOT\n",
    "        )\n",
    "    )\n",
    "\n",
    "def add_chunks_to_Qdrant_collection(embed_model, chunks, client, collection_name: str):\n",
    "    current_chunk_ID = client.count(collection_name).count + 1\n",
    "    for (chunk_ID, chunk) in enumerate(chunks, current_chunk_ID):\n",
    "        client.upsert(\n",
    "            collection_name = collection_name,\n",
    "            points = [get_transformed_chunk_as_PointStruct(embed_model, chunk, chunk_ID)],\n",
    "            wait = True\n",
    "        )\n",
    "\n",
    "def get_transformed_chunk_as_PointStruct(embed_model, chunk: dict, point_id: int):\n",
    "    point = PointStruct(\n",
    "        id = point_id,\n",
    "        vector = chunk[CHUNK_DATA_EMBEDDING_VECTOR_KEY],\n",
    "        payload = {\n",
    "            CHUNK_METADATA_ID_KEY: chunk[CHUNK_DATA_METADATA_KEY][CHUNK_METADATA_ID_KEY],\n",
    "            CHUNK_DATA_TEXT_KEY: chunk[CHUNK_DATA_TEXT_KEY],\n",
    "            CHUNK_DATA_METADATA_KEY: chunk[CHUNK_DATA_METADATA_KEY]\n",
    "        }\n",
    "    )\n",
    "    return point\n",
    "\n",
    "def get_chunks_texts_embed_vectors(embed_model, chunks):\n",
    "    chunks_texts = [chunk[CHUNK_DATA_TEXT_KEY] for chunk in chunks]\n",
    "    return embed_model.embed_documents(chunks_texts)\n",
    "\n",
    "def add_embed_vectors_to_chunks(chunks, embed_vectors: list[list]):\n",
    "    for (chunk_embed_vector, chunk) in zip(embed_vectors, chunks):\n",
    "        chunk[CHUNK_DATA_EMBEDDING_VECTOR_KEY] = chunk_embed_vector\n",
    "        \n",
    "def save_chunks_embedding_vectors(embed_vectors: list[np.array], filename: str):\n",
    "    with open(f\"{ARTIFACTS_DIRNAME}/{filename}\", 'wb') as embed_vectors_f:\n",
    "        pickle.dump(embed_vectors, embed_vectors_f)\n",
    "    print(f'Embedding Vectors Saved into {filename}!')\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Embeddings_Generation_and_Loading_into_Qdrant\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    main(QDRANT_CLIENT, \n",
    "         CHUNKS_COLLECTION_NAME,\n",
    "         CHUNKS_JSONL_FILENAME,\n",
    "         EMBEDDINGS_MODEL, \n",
    "         EMBEDDING_VECTOR_SIZE, \n",
    "         EMBEDDING_VECTORS_FILENAME\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunks Searching for User Query(Retrieve stage)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(qdrant_client, chunks_collection_name, embed_model, user_query: str):\n",
    "    vector_store = get_qdrant_vector_store(qdrant_client, chunks_collection_name, embed_model)\n",
    "    embedding_user_query = embed_model.embed_query(user_query)\n",
    "    user_answer_chunks = vector_store.similarity_search_by_vector(embedding_user_query, k = 2)\n",
    "    return user_answer_chunks\n",
    "\n",
    "def get_qdrant_vector_store(client, collection_name, embeddings_model):\n",
    "    return QdrantVectorStore.from_existing_collection(\n",
    "        host = QDRANT_HOST,\n",
    "        port = QDRANT_PORT,\n",
    "        collection_name = collection_name,\n",
    "        embedding = embeddings_model,\n",
    "        distance = Distance.DOT,\n",
    "        content_payload_key = CHUNK_DATA_TEXT_KEY,\n",
    "        metadata_payload_key = CHUNK_DATA_METADATA_KEY\n",
    "    )\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Chunks_Searching_for_User_Query(Retrieve stage)\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    chunks_for_LLM = main(\n",
    "        QDRANT_CLIENT,\n",
    "        CHUNKS_COLLECTION_NAME,\n",
    "        EMBEDDINGS_MODEL,\n",
    "        USER_QUERY\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Prompt Generation for LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(retrieved_chunks: list, user_query: str, llm_task_template: str):\n",
    "    context_text = get_context_text(retrieved_chunks)\n",
    "    system_role_message = get_system_role_message_for_prompt(llm_task_template, context_text)\n",
    "    user_role_message = get_user_role_message_for_prompt(user_query)\n",
    "    prompt = get_prompt(system_role_message, user_role_message)\n",
    "    return prompt\n",
    "\n",
    "def get_prompt(system_role_message, user_role_message) -> list[dict]:\n",
    "    prompt = [system_role_message, user_role_message]\n",
    "    return prompt\n",
    "\n",
    "def get_system_role_message_for_prompt(llm_task_template, context):\n",
    "    return {\"role\": \"system\", \"content\": f\"{llm_task_template}{context}\"}\n",
    "\n",
    "def get_user_role_message_for_prompt(question):\n",
    "    return {\"role\": \"user\", \"content\": question}\n",
    "\n",
    "def get_context_text(chunks: list):\n",
    "    context_text = '\\n'.join([chunk.page_content for chunk in chunks])\n",
    "    return context_text\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"Prompt Generation for LLM\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    prompt = main(chunks_for_LLM, USER_QUERY, LLM_TASK_TEMPLATE)\n",
    "\n",
    "mlflow.log_artifacts(ARTIFACTS_DIRNAME, artifact_path = f\"mlflow-artifacts:/{parent_run_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LLM Reply Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add trace for input-output tracking\n",
    "def main(llm_host, llm_port, model_name, prompt):\n",
    "    client = get_local_OpenAI_client(llm_host, llm_port)\n",
    "    answer = get_llm_answer(client, model_name, prompt)\n",
    "    return answer\n",
    "\n",
    "def get_local_OpenAI_client(host, port):\n",
    "    client = openai.OpenAI(\n",
    "        base_url = f\"http://{host}:{port}/v1\",\n",
    "        api_key = \"sk-no-key-required\"\n",
    "    )\n",
    "    return client\n",
    "    \n",
    "def get_llm_answer(client, model_name, prompt: list[dict]):\n",
    "    completion = client.chat.completions.create(\n",
    "        model = model_name,\n",
    "        messages = prompt,\n",
    "        max_completion_tokens = 30\n",
    "    )\n",
    "    return completion.choices[0].message\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name = \"LLM Reply Generation\",\n",
    "    experiment_id = experiment_ID,\n",
    "    nested = True\n",
    ") as child_run:\n",
    "    answer = main(\n",
    "        LLAMACPP_SERVER_HOST, \n",
    "        LLAMACPP_SERVER_PORT, \n",
    "        LLM_MODEL_NAME, \n",
    "        prompt\n",
    "    )\n",
    "    print(answer)\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RAG Files Versions Journal Creation from MLFlow Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_existed_parent_runs = mlflow.search_runs(\n",
    "    experiment_ID, \n",
    "    filter_string = \"\"\"tags.run_type = 'parent'\"\"\",\n",
    "    output_format = 'list'\n",
    ")\n",
    "\n",
    "def save_RAG_versions_journal(journal):\n",
    "    with open(f\"{ARTIFACTS_DIRNAME}/{RAG_VERSIONS_JOURNAL_NAME}\", 'w') as rag_v_f:\n",
    "        json.dump(journal, rag_v_f)\n",
    "\n",
    "def get_RAG_versions_journal(parent_runs: list, experiment_name) -> list[dict]:\n",
    "    rag_files_versions_journal = []\n",
    "    experiment_artifacts_URI = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME).artifact_location\n",
    "    parent_runs_article_texts = []\n",
    "    for parent_run in parent_runs:\n",
    "        parent_run_version = parent_run.data.tags['parent_version']\n",
    "        parent_run_artifacts_files_URI = f\"{parent_run.info.artifact_uri}/mlflow-artifacts:/{parent_run_version}/\"\n",
    "        parent_run_artifacts = mlflow.artifacts.list_artifacts(parent_run_artifacts_files_URI)\n",
    "        parent_run_artifacts_json = {'rag_files_version': parent_run_version, 'artifacts': []}\n",
    "        for run_artifact_file in parent_run_artifacts:\n",
    "            artifact_URI = parent_run_artifacts_files_URI + run_artifact_file.path\n",
    "            artifact_info = {'artifact_filename': run_artifact_file.path, 'artifact_URI': artifact_URI}\n",
    "            parent_run_artifacts_json['artifacts'].append(artifact_info)\n",
    "        rag_files_versions_journal.append(parent_run_artifacts_json)\n",
    "    return rag_files_versions_journal\n",
    "\n",
    "RAG_versions_journal = get_RAG_versions_journal(experiment_existed_parent_runs, MLFLOW_EXPERIMENT_NAME)\n",
    "save_RAG_versions_journal(RAG_versions_journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
